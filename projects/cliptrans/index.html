<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation">
  <meta name="keywords" content="CLIP, mBART, NMT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://devaansh100.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://devaansh100.github.com">Devaansh Gupta*</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="">Siddhant Kharbanda</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="">Jiawei Zhou</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="">Wanhua Li</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://seas.harvard.edu/person/hanspeter-pfister">Hanspeter Pfister</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="http://donglaiw.github.io">Donglai Wei</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Boston College</span>
            <span class="author-block"><sup>2</sup>BITS Pilani, Pilani</span>
            <span class="author-block"><sup>3</sup>Microsoft, India</span>
            <span class="author-block"><sup>4</sup>Harvard University</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">*Corresponding Author</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">Published in International Conference of Computer Vision, 2023</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/devaansh100/cliptrans"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
          <div class="container is-max-desktop">
            <div class="container has-text-centered">
              <figure class="image is-50x-auto is-inline-block">
                <img src="./static/images/teaser.jpg" alt="Centered Image" style="max-width: 60%; display: block; margin: 0 auto;">
                <h2 class="subtitle has-text-centered">
                  We aim to enhance machine translation by using images in training. For transfer learning approaches, we need to initialise the weights with a pre-trained multilingual multimodal model. However, existing pre-trained models are either multilingual or multimodal.
              </h2>
              </figure>
            </div>
          </div>
        </div>
      </div>
</section>

<section class="hero teaser mt-6">
  <div class="container is-max-desktop">
    <div class="container has-text-centered">
      <figure class="image is-inline-block">
        <img src="./static/images/model_overall.pdf" alt="Centered Image">
        <h2 class="subtitle has-text-centered">
          We thus propose CLIPTrans, which can combine existing pairwise models in a data efficient manner. This is done by first aligning the models with a mapping network on a captioning task, followed by thr actual translation task. We find that this beats finetuning text-only machine translation, and furthers the SOTA on multimodal machine translation.
      </h2>
      </figure>
    </div>
  </div>
</section>

<section class="hero teaser mt-6">
  <div class="container is-max-desktop">
    <div class="container has-text-centered">
      <figure class="image is-inline-block">
        <img src="./static/images/captioning.pdf" alt="Centered Image">
        <h2 class="subtitle has-text-centered">
          Does captioning really help? We show this qualitatively. Inference is run with two different inputs: images(whose output is "caption") and german source texts(whose output is "translation"). Captions capture the gist needed for translation, however, the exact words to be used can only be known by finetuning on source texts. Notably, even without finetuning, CLIPTrans can translate some of the semantics of german text without seeing paired texts. This also motivates the second translation task.
      </h2>
      </figure>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           There has been a growing interest in developing multimodal machine translation (MMT) systems that use images as auxiliary information for neural machine translation(NMT). While NMT has seen advancements through transfer learning approaches, the same cannot be said for MMT. This is primarily because MMT requires multimodal multilingual pre-trained models which require annotated images with multilingual descriptions to train. Such data is not available at the necessary scale, especially for low-resource languages. 
         </p>
         <p>
           To circumvent this issue, and facilitate the use of pre-trained models in MMT, we propose CLIPTrans, a new MMT architecture that transfers the multimodal representations of M-CLIP into a multilingual mBART. M-CLIP embeddings are introduced into mBART through a prefix sequence generated by a lightweight mapping network. Furthermore, we introduce an iterative two-stage pipeline which warms up the model with image captioning before the actual translation task. Through various experiments, we demonstrate the importance of this and consequently push forward the state-of-the-art across various benchmarks by an average of +2.67 BLEU, without using images at test time. Overall, we simplify the MMT pipeline by eliminating the need for engineered attention modules and intractable auxiliary losses for vision-language alignment.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
          <p>Coming soon!</p>
        <!-- <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div> -->
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{gupta2023cliptrans,
  author    = {Gupta, Devaansh and Kharbanda, Siddhant and Zhou, Jiawei and Li, Wanhua and Pfister, Hanspeter and Wei, Donglai},
  title     = {Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation},
  journal   = {ICCV},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/devaansh100" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to <a href="https://nerfies.github.io">this</a> page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
